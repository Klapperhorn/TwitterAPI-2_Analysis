{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "#import yaml\n",
    "import time\n",
    "\n",
    "from myTwitterLibrary import myTwitterLibrary as mtl\n",
    "\n",
    "Target_File_Name='ChatGPT'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Suchfrage='(from:Nuklearia OR from:VeroWendland) lang:de'\n",
    "#Suchfrage='(from:ausgestrahlt OR from:atomradar OR from:janhaverkamp OR from:XAhausX OR from:Urantransport) lang:de'\n",
    "#Suchfrage=\"NanoBubbles OR @_Nano_Bubbles OR _Nano_Bubbles\"\n",
    "#Suchfrage='(from:BASE_bund OR from:strahlenschutz) lang:de'\n",
    "#Suchfrage='(Atomkraft OR Atomenergie OR Atomreaktor OR Kernenergie OR Kernkraft OR Kernreaktor OR SMR) (Klima OR Klimawandel OR Klimaschutz)  lang:de'\n",
    "#Suchfrage='(Endlager OR Endlagersuche OR AtommÃ¼ll) (Fachkonferenz OR Teilgebiete OR Beteiligung OR beteiligen)  lang:de'\n",
    "#SearchTerm=\"(#aiethics OR #sustainableAI OR #inclusiveai OR #greendata OR #responsibleai OR #trustworthyai OR #aiforgood OR #creativeai OR 3greenai OR #impactai OR #ai4prosperity OR #dataecosystems) -is:retweet\"\n",
    "#SearchTerm=\"(technologyassessment OR Technikfolgen OR (technology assessment)) -is:retweet\"\n",
    "#SearchTerm=\"Technikfolgen OR #TechnologyAssessment -is:retweet\"\n",
    "#SearchTerm=\"#BigData\"\n",
    "#SearchTerm='#GreenAI -is:retweet'\n",
    "#SearchTerm=\"(#sustainableAI OR #inclusiveai OR #greendata OR #greenai)\"\n",
    "SearchTerm=\"(#AI (#sustainability OR #sustainable)) OR #SustainableAI OR #GreenAI\" ### 2023-03-02\n",
    "#SearchTerm=\"(GPT3, openAI OR ChatGPT OR ChatGPT3 OR GPT)\"\n",
    "#SearchTerm=\"(#GPT3, #openAI OR #ChatGPT OR #ChatGPT3 OR #GPT) (education OR uni OR university OR students OR school OR exam OR teaching OR teacher OR highschool) -is:retweet\"\n",
    "\n",
    "\n",
    "### Twitter Search\n",
    "\n",
    "\n",
    "\n",
    "start_time='2010-01-01'\n",
    "end_time='2023-03-30'\n",
    "\n",
    "\n",
    "max_results=500 \n",
    "query_params = {'query': SearchTerm,\n",
    "                'start_time': mtl.dtC(start_time),\n",
    "                'end_time': mtl.dtC(end_time),\n",
    "                #'since_id': since_id,\n",
    "                'max_results': 500,          # tweets per file --> important for large files\n",
    "                'tweet.fields': 'author_id,created_at,entities,lang,public_metrics,in_reply_to_user_id,referenced_tweets,conversation_id,attachments',\n",
    "                'user.fields': 'created_at,description,entities,id,location,name,username',\n",
    "                'media.fields':'type,alt_text,url,height,preview_image_url',\n",
    "                'expansions':'attachments.media_keys,referenced_tweets.id,referenced_tweets.id.author_id,entities.mentions.username,in_reply_to_user_id'\n",
    "                }\n",
    "\n",
    "\n",
    "#Target_File_Name=\"\".join(filter(str.isalpha, SearchTerm.split(' ')[0]))\n",
    "query_params\n",
    "\n",
    "print(Target_File_Name, query_params)\n",
    "\n",
    "\n",
    "\n",
    "startnumber=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for VersuchsNummer in range(5):\n",
    "    try:\n",
    "        for i in range(startnumber,1500): #100 bedeutet maximal 50.000 Tweets\n",
    "\n",
    "            startnumber=i ## allows to restart  with the next number\n",
    "            print(i,i*max_results)\n",
    "            json_response=mtl.main(query_params)\n",
    "            time.sleep(0.5)  \n",
    "            NextToken=json_response['meta']['next_token']\n",
    "            query_params[\"next_token\"]=NextToken\n",
    "\n",
    "            print(json_response[\"meta\"])\n",
    "\n",
    "            with open(mtl.Today+\"_M_\"+Target_File_Name+\"__\"+str(i)+'_data'+'.json', 'w', encoding='utf-8') as f:\n",
    "                json.dump(json_response, f, ensure_ascii=False, indent=4)\n",
    "    except:\n",
    "        print(\"retry :D :D :D\", VersuchsNummer,i)\n",
    "        time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REAL_Next_Token=query_params[\"next_token\"] ### Number 443\n",
    "REAL_Next_Token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search with the Count engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time='2021-10-01'  ### Earliest: 2006-03-21T00:00Z\n",
    "end_time='2023-04-07'\n",
    "#SearchTerm='NanoBubbles OR @_Nano_Bubbles OR _Nano_Bubbles'\n",
    "\n",
    "\n",
    "# Optional params: start_time,end_time,since_id,until_id,next_token,granularity\n",
    "query_params = {'query': SearchTerm,\n",
    "                'granularity': 'day', \n",
    "                'start_time': mtl.dtC(start_time),\n",
    "                'end_time': mtl.dtC(end_time),\n",
    "               }\n",
    "\n",
    "print(query_params)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResultList=[]\n",
    "for i in range(500): #100 bedeutet maximal 50.000 Tweets\n",
    "    print(i)\n",
    "    json_response=mtl.mainCount(query_params)\n",
    "    time.sleep(0.05)\n",
    "    Data=json_response[\"data\"]\n",
    "    \n",
    "    NextToken=json_response['meta']['next_token']\n",
    "    \n",
    "    query_params[\"next_token\"]=NextToken\n",
    "    print(NextToken)\n",
    "    ResultList.extend(Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(ResultList)\n",
    "df[\"Date\"]=df[\"start\"].apply(pd.to_datetime)\n",
    "df.set_index(\"Date\", inplace=True)\n",
    "\n",
    "df[\"tweet_count\"].to_csv(mtl.Today+\"_\"+Target_File_Name+\"_Count_results.csv\")\n",
    "\n",
    "from textwrap import wrap\n",
    "\n",
    "sampleSize=\"M\"\n",
    "\n",
    "PlotTitle=\"\\n\".join(wrap(f\"Tweets per {sampleSize} on {SearchTerm} between {start_time} and {end_time}\"))\n",
    "sample=df.resample(sampleSize).sum()\n",
    "ax=sample.plot(title=PlotTitle, figsize=(17,4))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title(PlotTitle, wrap=True)\n",
    "\n",
    "plt.tight_layout(pad=1.08, h_pad=None, w_pad=None, rect=None)\n",
    "plt.savefig(f\"{mtl.Today}_{Target_File_Name}_2_History_tweets.pdf\", dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Nanotechnology=pd.read_csv(\"22-06-14_nanotechnology_Count_results.csv\", index_col=\"Date\", converters={\"Date\":pd.to_datetime})\n",
    "df_Nanotechnology.plot()\n",
    "\n",
    "df_3D=pd.read_csv(\"22-06-14_3d printing_Count_results.csv\", index_col=\"Date\", converters={\"Date\":pd.to_datetime})\n",
    "df_3D.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleSize=\"W\"\n",
    "CompTechs=\"Nano & 3D Printing\"\n",
    "\n",
    "compare=compare.join(df_3D, how='left', on=\"Date\", lsuffix=' nanotechnology', rsuffix=' 3d-printing')\n",
    "\n",
    "sample=compare.resample(sampleSize).sum()\n",
    "ax=sample.plot(title=f\"Tweets per {sampleSize} on {CompTechs} between {start_time} and {end_time}\", figsize=(14,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleSize=\"W\"\n",
    "CompTechs=\"Nanotechnology & 3D Printing & NanoParticles\"\n",
    "\n",
    "compare=compare.join(df[\"tweet_count\"], how='left', on=\"Date\", rsuffix=' Nanoparticle')\n",
    "\n",
    "sample=compare.resample(sampleSize).sum()\n",
    "ax=sample.plot(title=f\"Tweets per {sampleSize} on {CompTechs} between {start_time} and {end_time}\", figsize=(14,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax=sample.plot(title=f\"Tweets per {sampleSize} on {CompTechs} between {start_time} and {end_time}\", figsize=(14,3))\n",
    "\n",
    "CompTechs=\"Nanotechnology-3D Printing-NanoParticles\"\n",
    "\n",
    "plt.tight_layout(pad=1.08, h_pad=None, w_pad=None, rect=None)\n",
    "plt.savefig(f\"{Heute}_{CompTechs}_History_tweets.jpg\", dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.loc['2020-1-1':'2020-06-30'].plot()\n",
    "\n",
    "### What happened March 2020 ? Covid?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Results / First look at data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(json_response['data'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Combine Tweet Collections\n",
    "\n",
    "combined_results = []\n",
    "\n",
    "import glob\n",
    "dataList=glob.glob(\"*\"+DataName+\"*\"+\"_data.json\")\n",
    "print(DataName)\n",
    "with open(Heute+\"_\"+DataName+'combined_results.json', 'w', encoding='utf-8') as outfile:\n",
    "    for file in dataList:\n",
    "        f = open(file, 'r', encoding='utf-8')\n",
    "        combined_results.append(json.load(f)[\"data\"])\n",
    "    json.dump(combined_results, outfile)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame()\n",
    "data = json.load(open('22-02-09_NanoBiologycombined_results.json', encoding=\"utf8\"))\n",
    "for i in range(len(data)):\n",
    "    dfx=pd.DataFrame(data[i])\n",
    "    df=pd.concat(objs=[df,dfx])\n",
    "    #df=df.append(dfx) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(json_response[\"data\"])\n",
    "df[\"entities\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractEntitites(entities):\n",
    "    hashtagsListe, UrlListe, mentionListe,annotationsListe=[],[],[],[]\n",
    "    #print(entities)\n",
    "    if entities==entities:\n",
    "        if \"hashtags\" in entities:\n",
    "                hashtagsListe=[i[\"tag\"] for i in entities[\"hashtags\"] ]\n",
    "        if 'urls' in entities:\n",
    "            UrlListe=[i['expanded_url'] for i in entities['urls']]\n",
    "        if \"mentions\" in entities:\n",
    "            mentionListe=[i['username'] for i in entities[\"mentions\"] ]\n",
    "        if 'annotations' in entities:\n",
    "            annotationsListe=[(i['type'],i['normalized_text']) for i in entities['annotations'] ]  \n",
    "        res=(hashtagsListe, UrlListe, mentionListe, annotationsListe)\n",
    "        return res\n",
    "\n",
    "hashtagsListe, UrlListe, mentionListe,annotationsListe=zip(*df[\"entities\"].apply(extractEntitites))\n",
    "#df[\"hashtagsListe\"], df[\"UrlListe\"], df[\"mentionListe\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities={\"hashtagsListe\":hashtagsListe, \"UrlListe\":UrlListe, 'mentionListe': mentionListe}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hl=entities['hashtagsListe']\n",
    "hashtags=[item for sublist in hl for item in sublist]\n",
    "from collections import Counter\n",
    "Counter(hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "flat_list = [item.title().strip() for sublist in hashtagsListe for item in sublist]\n",
    "a_counter = Counter(flat_list)\n",
    "\n",
    "most_common = a_counter.most_common(50)\n",
    "\n",
    "wordcloud=WordCloud(background_color=\"white\", width=1200, height=1200).generate_from_frequencies(dict(most_common))\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(Heute+DataName+\"_WordCloud.pdf\",dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common = a_counter. most_common(15)[1:]\n",
    "dhash=pd.DataFrame(most_common,columns=[\"Hashtag\",\"counts\"])\n",
    "\n",
    "plt.figure()\n",
    "dhash.set_index(\"Hashtag\").plot.barh(figsize=(3,4),title=\"Co-Hashtags zu: \"+Suchfrage)\n",
    "\n",
    "plt.savefig(Heute+DataName+\"_Hashtag-Barh.jpg\",dpi=300,bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
