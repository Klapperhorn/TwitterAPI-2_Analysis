{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f52aac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#import requests\n",
    "import json\n",
    "#import yaml\n",
    "import glob\n",
    "\n",
    "from myTwitterLibrary import FileImport\n",
    "from myTwitterLibrary import nlp\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc3f8a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Json Files:  2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'public_metrics': {'retweet_count': 0,\n",
       "  'reply_count': 0,\n",
       "  'like_count': 2,\n",
       "  'quote_count': 0,\n",
       "  'impression_count': 205},\n",
       " 'created_at': '2023-03-29T21:22:17.000Z',\n",
       " 'lang': 'en',\n",
       " 'edit_history_tweet_ids': ['1641189056511119360'],\n",
       " 'entities': {'hashtags': [{'start': 93, 'end': 101, 'tag': 'ChatGPT'},\n",
       "   {'start': 102, 'end': 105, 'tag': 'AI'},\n",
       "   {'start': 106, 'end': 113, 'tag': 'Boston'},\n",
       "   {'start': 114, 'end': 128, 'tag': 'journorequest'}]},\n",
       " 'id': '1641189056511119360',\n",
       " 'text': 'Looking for speaking to Boston students talking about the experience of playing with Chatgpt #ChatGPT #AI #Boston #journorequest',\n",
       " 'conversation_id': '1641189056511119360',\n",
       " 'author_id': '1050835728232587264'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "DataName=\"chatGPT\"\n",
    "\n",
    "\n",
    "dataList=glob.glob(\"*\"+DataName+\"*\"+\"_data.json\")\n",
    "dataList\n",
    "print(\"Number of Json Files: \", len(dataList))\n",
    "x=FileImport.load_data(dataList[0])\n",
    "\n",
    "#example Data\n",
    "\n",
    "x[\"data\"][16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a0bb8395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3                                       My latest blog about the support that offers students with\n",
       "4        years ago, how has it changed? The Growing Need for Skills in Artificial Intelligence via\n",
       "5    All my time ranting about law schools not teaching lawyering has been for naught. Thanks, . /\n",
       "Name: clean_text, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfk.clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "83b28e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=dfk.clean_text[5]\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe(\"spacytextblob\")\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0f3736f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n"
     ]
    }
   ],
   "source": [
    "print(doc._.blob.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cd0a50bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My latest blog about the support that offers students with\n",
      "years ago, how has it changed? The Growing Need for Skills in Artificial Intelligence via\n",
      "All my time ranting about law schools not teaching lawyering has been for naught. Thanks, . /\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.6</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1\n",
       "3  0.3  0.45\n",
       "4 -0.6  1.00\n",
       "5  0.2  0.20"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfk=df.iloc[3:6,:]\n",
    "\n",
    "dfk.clean_text\n",
    "\n",
    "import spacy\n",
    "\n",
    "\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "\n",
    "def Sentiment(text,nlp):\n",
    "    \n",
    "    nlp=spacy.load('en_core_web_sm')\n",
    "    nlp.add_pipe('spacytextblob')\n",
    "    doc = nlp(text)\n",
    "    print(doc)\n",
    "    polarity=doc._.blob.polarity\n",
    "    subjectivity=doc._.blob.subjectivity\n",
    "    return pd.Series([polarity, subjectivity])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dfk.clean_text.apply(Sentiment, nlp=spacy.load('en_core_web_sm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "350ae141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "2023-03-30_M_ChatGPT__0_data.json, 2023-03-30_M_ChatGPT__1_data.json\n",
      "Tweets: 1000, Media:206, Users: 1164\n",
      "cleaning done.\n",
      "language detection done.\n",
      "pure english text done. Next: Token & Lemmatizing.\n",
      "Token & Lemmatizing done. Next: Remove Stopwords.\n",
      "Stopwording done. Next: sentiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['text', 'text_clean', 'created_at', 'id', 'author_id', 'retweet_count',\n",
       "       'reply_count', 'like_count', 'quote_count', 'impression_count',\n",
       "       'word_count', 'attachments', 'urls', 'hashtags', 'mentions',\n",
       "       'media_url', 'username', 'name', 'description', 'location', 'RT',\n",
       "       'clean_text', 'letters_count', 'language', 'pure_text', 'Lemmata',\n",
       "       'NoStopwords', 'polarity', 'subjectivity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from myTwitterLibrary import FileImport\n",
    "from myTwitterLibrary import nlp\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "DataName=\"ChatGpt\"\n",
    "\n",
    "combined_results,combined_media,combined_user,combined_extTweets=FileImport.FileImport(DataName)\n",
    "df=FileImport.prepare_df(combined_results,combined_media,combined_user, Simplify=True)\n",
    "df=nlp.NLP_Pipeline(df, sentiment=True, language=\"en\")\n",
    "\n",
    "df.to_json(FileImport.Today+DataName+\"_all.json\")\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96450b81",
   "metadata": {},
   "source": [
    "# Only add the Sentiment NLP to existing Json Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ec8380d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "\n",
    "#Filenames to rework\n",
    "\n",
    "Files=[]#['2023-01-22GPT_EDU.json',\"2023-01-25nanoMedicine.json\",\"2023-01-25sustain.json\"]\n",
    "\n",
    "for file in Files:\n",
    "    \n",
    "    now = datetime.now()\n",
    "\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    print(\"Current Time =\", current_time)\n",
    "    \n",
    "    print(\"starting with: \", file)\n",
    "    #df=pd.read_json(file)\n",
    "    df[[\"polarity\",\"subjectivity\"]]=df.clean_text.apply(nlp.Sentiment)\n",
    "    print(\"writing: \", file)\n",
    "    df.to_json(FileImport.Today+\"_\"+file+\".json\")\n",
    "    print(file,\"done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
